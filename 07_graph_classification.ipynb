{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "import torch.optim as optim\r\n",
    "import torch.nn.functional as F\r\n",
    "\r\n",
    "import dgl\r\n",
    "import dgl.data\r\n",
    "import dgl.nn as gnn\r\n",
    "\r\n",
    "from dgl.dataloading import GraphDataLoader\r\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Задача: предсказать какую-нибудь характеристику на уровне целого графа\r\n",
    "\r\n",
    "Используем синтетический датасет GIN. \r\n",
    "Он содержит некоторое кол-во графов, у каждого графа есть \r\n",
    "а) фичи на узлах и \r\n",
    "б) метка на весь граф "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "dataset = dgl.data.GINDataset('PROTEINS', self_loop=True)\r\n",
    "print('Node feature dimensionality:', dataset.dim_nfeats)\r\n",
    "print('Number of graph categories:', dataset.gclasses)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Node feature dimensionality: 3\n",
      "Number of graph categories: 2\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Проблема: нужно уметь разбивать датасет на мини-батчи. Решение: `GraphDataLoader`, который работает со стандартными сэмплерами из `torch`.\r\n",
    "\r\n",
    "Каждый элемент в датасете представляет собой пару (граф, метка). `GraphDataLoader` при итерации по нему возвращает два объекта: объедененный граф для батча и вектор с метками для каждого графа из минибатча. \r\n",
    "\r\n",
    "Объединенный граф состоит из нескольких графов, объединенных в один в виде несвязных компонент. Фичи узлов и ребер сохраняются. Это такой же граф, как и обычный `DGLGraph`, но содержит доп. информацию для восстановления исходных графов. Развернуть графы назад можно с помощью метода `unbatch`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "num_examples = len(dataset)\r\n",
    "num_train = int(num_examples * .8)\r\n",
    "\r\n",
    "train_sampler = SubsetRandomSampler(torch.arange(num_train))\r\n",
    "test_sampler = SubsetRandomSampler(torch.arange(num_train, num_examples))\r\n",
    "\r\n",
    "train_dataloader = GraphDataLoader(dataset, sampler=train_sampler, batch_size=5, drop_last=False)\r\n",
    "test_dataloader = GraphDataLoader(dataset, sampler=test_sampler, batch_size=5, drop_last=False)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "it = iter(train_dataloader)\r\n",
    "batched_graph, labels = next(it)\r\n",
    "print('Number of nodes for each graph element in the batch:',\r\n",
    "      batched_graph.batch_num_nodes())\r\n",
    "print('Number of edges for each graph element in the batch:',\r\n",
    "      batched_graph.batch_num_edges())\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of nodes for each graph element in the batch: tensor([ 16,  14,  27,  31, 125])\n",
      "Number of edges for each graph element in the batch: tensor([ 80,  64, 129, 145, 553])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# Recover the original graph elements from the minibatch\r\n",
    "graphs = dgl.unbatch(batched_graph)\r\n",
    "print('The original graphs in the minibatch:')\r\n",
    "print(graphs)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The original graphs in the minibatch:\n",
      "[Graph(num_nodes=16, num_edges=80,\n",
      "      ndata_schemes={'label': Scheme(shape=(), dtype=torch.int64), 'attr': Scheme(shape=(3,), dtype=torch.float32)}\n",
      "      edata_schemes={}), Graph(num_nodes=14, num_edges=64,\n",
      "      ndata_schemes={'label': Scheme(shape=(), dtype=torch.int64), 'attr': Scheme(shape=(3,), dtype=torch.float32)}\n",
      "      edata_schemes={}), Graph(num_nodes=27, num_edges=129,\n",
      "      ndata_schemes={'label': Scheme(shape=(), dtype=torch.int64), 'attr': Scheme(shape=(3,), dtype=torch.float32)}\n",
      "      edata_schemes={}), Graph(num_nodes=31, num_edges=145,\n",
      "      ndata_schemes={'label': Scheme(shape=(), dtype=torch.int64), 'attr': Scheme(shape=(3,), dtype=torch.float32)}\n",
      "      edata_schemes={}), Graph(num_nodes=125, num_edges=553,\n",
      "      ndata_schemes={'label': Scheme(shape=(), dtype=torch.int64), 'attr': Scheme(shape=(3,), dtype=torch.float32)}\n",
      "      edata_schemes={})]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Workflow:\r\n",
    "1. Создаем как обычно двухслойную сеть. На вход придет батч-граф. \r\n",
    "2. Дальше нужно сагрегировать представления узлов (и возможно ребер) чтобы получить представление графа в целом (самый простой вариант - усреднить с помощью `dgl.mean_nodes()`). Этот процесс называют `readout`. `DGL` предоставляет набор функций, которые могут работать с батч-графами и получать для них представление. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "class GCN(nn.Module):\r\n",
    "\r\n",
    "    def __init__(self, n_inputs, n_hidden, num_classes):\r\n",
    "        super().__init__()\r\n",
    "        self.conv1 = gnn.GraphConv(n_inputs, n_hidden)\r\n",
    "        self.conv2 = gnn.GraphConv(n_hidden, num_classes)\r\n",
    "\r\n",
    "    def forward(self, G, features):\r\n",
    "        out = F.relu(self.conv1(G, features))\r\n",
    "        out = self.conv2(G, out)\r\n",
    "        G.ndata['h'] = out\r\n",
    "        return dgl.mean_nodes(G, 'h')\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "n_inputs, n_hidden, n_out = dataset.dim_nfeats, 16, dataset.gclasses\r\n",
    "model = GCN(n_inputs, n_hidden, n_out)\r\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=.01)\r\n",
    "\r\n",
    "n_epochs = 20\r\n",
    "for epoch in range(n_epochs):\r\n",
    "    for batched_graph, labels in train_dataloader:\r\n",
    "        pred = model(batched_graph, batched_graph.ndata['attr'].float())\r\n",
    "        loss = F.cross_entropy(pred, labels)\r\n",
    "        optimizer.zero_grad()\r\n",
    "        loss.backward()\r\n",
    "        optimizer.step()\r\n",
    "\r\n",
    "num_correct = 0\r\n",
    "num_tests = 0\r\n",
    "for batched_graph, labels in test_dataloader:\r\n",
    "    pred = model(batched_graph, batched_graph.ndata['attr'].float())\r\n",
    "    num_correct += (pred.argmax(1) == labels).sum().item()\r\n",
    "    num_tests += len(labels)\r\n",
    "\r\n",
    "print('Test accuracy:', num_correct / num_tests)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Test accuracy: 0.24663677130044842\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('gcn': conda)"
  },
  "interpreter": {
   "hash": "a5d512aadeb6aaabeaf2234f39752bedcd0c0a511f474f5ce732b89a77e68aba"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}