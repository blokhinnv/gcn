{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\r\n",
    "\r\n",
    "import numpy as np\r\n",
    "import scipy.sparse as sp\r\n",
    "from sklearn.metrics import roc_auc_score\r\n",
    "\r\n",
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "import torch.optim as optim\r\n",
    "import torch.nn.functional as F\r\n",
    "\r\n",
    "import dgl\r\n",
    "import dgl.data\r\n",
    "import dgl.nn as gnn\r\n",
    "import dgl.function as fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Workflow:\r\n",
    "1. Имеющиеся связи - положительные примеры\r\n",
    "2. Нужно насэплить несуществующих связей - это будут негативные примеры. Задача предсказания связей состоит в сравнении оценок вероятности существования связи между парами узлов, которые действительно связаны, и между парами узлов, выбранных случайно. \r\n",
    "3. Разделить полученные примеры на обучающую и тестовую выборку\r\n",
    "4. Решить задачу бинарной классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NumNodes: 2708\n",
      "  NumEdges: 10556\n",
      "  NumFeats: 1433\n",
      "  NumClasses: 7\n",
      "  NumTrainingSamples: 140\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done loading data from cached files.\n"
     ]
    }
   ],
   "source": [
    "dataset = dgl.data.CoraGraphDataset()\r\n",
    "G = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаем обучающую и тестовую выборку\r\n",
    "u, v = G.edges()\r\n",
    "\r\n",
    "# positive triples\r\n",
    "eids = np.random.permutation(np.arange(G.num_edges()))\r\n",
    "test_size = int(len(eids) * .1)\r\n",
    "train_size = len(eids) - test_size\r\n",
    "\r\n",
    "test_pos_u, test_pos_v = u[:test_size], v[:test_size]\r\n",
    "train_pos_u, train_pos_v = u[test_size:], v[test_size:]\r\n",
    "\r\n",
    "# negative triples\r\n",
    "# adj[i, j] == 1 iif i->v\r\n",
    "adj = sp.coo_matrix((np.ones(len(u)), (u.numpy(), v.numpy())))\r\n",
    "# adj_neg[i, j] == 1 iif adj[i, j] = 0 and i != j\r\n",
    "adj_neg = 1 - adj.todense() - np.eye(G.number_of_nodes())\r\n",
    "neg_u, neg_v = np.where(adj_neg != 0)\r\n",
    "\r\n",
    "neg_eids = np.random.choice(len(neg_u), G.number_of_edges())\r\n",
    "test_neg_u, test_neg_v = neg_u[neg_eids[:test_size]], neg_v[neg_eids[:test_size]]\r\n",
    "train_neg_u, train_neg_v = neg_u[neg_eids[train_size:]], neg_v[neg_eids[train_size:]]\r\n",
    "\r\n",
    "# удаляем ребра из тестового множества\r\n",
    "train_G = dgl.remove_edges(G, eids[:test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\r\n",
    "    '''Для каждого узла возвращает вектор (эмбеддинг) длины n_hidden. На основе близости двух эмбеддингов узлов\r\n",
    "    принимается решение о том, существует связь или нет'''\r\n",
    "    def __init__(self, n_inputs, n_hidden):\r\n",
    "        super().__init__()\r\n",
    "        self.conv1 = gnn.SAGEConv(n_inputs, n_hidden, aggregator_type='mean')\r\n",
    "        self.conv2 = gnn.SAGEConv(n_hidden, n_hidden, aggregator_type='mean')\r\n",
    "\r\n",
    "    def forward(self, G, features):\r\n",
    "        out = F.relu(self.conv1(G, features))\r\n",
    "        out = self.conv2(G, out)\r\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotPredictor(nn.Module):\r\n",
    "    def forward(self, G, features):\r\n",
    "        with G.local_scope():\r\n",
    "            G.ndata['h'] = features\r\n",
    "            G.apply_edges(fn.u_dot_v('h', 'h', 'score'))\r\n",
    "            return G.edata['score'].squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для работы создадим 4 графа: 2 из них на основе обучающего и тестового множества, содержащих положительные примеры; и 2 - на основе множеств, содержащих негативные примеры. Для удобства явно указываем, что в каждом из них одинаковое кол-во вершин.\r\n",
    "\r\n",
    "Имея на руках графы, можем воспользоваться методом `apply_edges` и быстренько посчитать фичи для ребер на основе эмбеддингов узлов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos_G = dgl.graph((train_pos_u, train_pos_v), num_nodes=G.num_nodes())\r\n",
    "train_neg_G = dgl.graph((train_neg_u, train_neg_v), num_nodes=G.num_nodes())\r\n",
    "\r\n",
    "test_pos_G = dgl.graph((test_pos_u, test_pos_v), num_nodes=G.num_nodes())\r\n",
    "test_neg_G = dgl.graph((test_neg_u, test_neg_v), num_nodes=G.num_nodes())\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 0, loss: 0.6911070942878723\n",
      "In epoch 5, loss: 0.36575984954833984\n",
      "In epoch 10, loss: 0.3100879490375519\n",
      "In epoch 15, loss: 0.3257092535495758\n",
      "In epoch 20, loss: 0.30938130617141724\n",
      "In epoch 25, loss: 0.28581932187080383\n",
      "In epoch 30, loss: 0.28284579515457153\n",
      "In epoch 35, loss: 0.2713790833950043\n",
      "In epoch 40, loss: 0.2587047219276428\n",
      "In epoch 45, loss: 0.24599483609199524\n",
      "In epoch 50, loss: 0.2322506606578827\n",
      "In epoch 55, loss: 0.21729446947574615\n",
      "In epoch 60, loss: 0.20117148756980896\n",
      "In epoch 65, loss: 0.18405020236968994\n",
      "In epoch 70, loss: 0.16695138812065125\n",
      "In epoch 75, loss: 0.1509188413619995\n",
      "In epoch 80, loss: 0.13618172705173492\n",
      "In epoch 85, loss: 0.12221866846084595\n",
      "In epoch 90, loss: 0.10906071215867996\n",
      "In epoch 95, loss: 0.09686066210269928\n",
      "AUC 0.707791828575279\n"
     ]
    }
   ],
   "source": [
    "n_inputs = G.ndata['feat'].shape[1]\r\n",
    "n_hidden = 16\r\n",
    "n_epochs = 100\r\n",
    "model = GCN(n_inputs, n_hidden)\r\n",
    "predictor = DotPredictor()\r\n",
    "\r\n",
    "def compute_loss(pos_score, neg_score):\r\n",
    "    scores = torch.cat([pos_score, neg_score])\r\n",
    "    labels = torch.cat([torch.ones_like(pos_score), torch.zeros_like(neg_score)])\r\n",
    "    return F.binary_cross_entropy_with_logits(scores, labels)\r\n",
    "\r\n",
    "def compute_auc(pos_score, neg_score):\r\n",
    "    scores = torch.cat([pos_score, neg_score])\r\n",
    "    labels = torch.cat([torch.ones_like(pos_score), torch.zeros_like(neg_score)])\r\n",
    "    return roc_auc_score(labels, scores)\r\n",
    "\r\n",
    "optimizer = optim.Adam(chain(model.parameters(), predictor.parameters()), lr=.01)\r\n",
    "\r\n",
    "for epoch in range(n_epochs):\r\n",
    "    # forward\r\n",
    "    embeddings = model(train_G, train_G.ndata['feat'])\r\n",
    "    pos_score = predictor(train_pos_G, embeddings)\r\n",
    "    neg_score = predictor(train_neg_G, embeddings)\r\n",
    "    loss = compute_loss(pos_score, neg_score)\r\n",
    "    # backward\r\n",
    "    loss.backward()\r\n",
    "    optimizer.step()\r\n",
    "    optimizer.zero_grad()\r\n",
    "\r\n",
    "    if not epoch % 5:\r\n",
    "        print('In epoch {}, loss: {}'.format(epoch, loss))\r\n",
    "with torch.no_grad():\r\n",
    "    pos_score = predictor(test_pos_G, embeddings)\r\n",
    "    neg_score = predictor(test_neg_G, embeddings)\r\n",
    "    print('AUC', compute_auc(pos_score, neg_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Еще один вариант negative sampling\r\n",
    "Отличия:\r\n",
    "1. Нет разделения на train/test\r\n",
    "2. Для каждого позитивного примера строится `k` негативных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_negative_graph(G, k):\r\n",
    "    u, v = G.edges()\r\n",
    "    neg_u = u.repeat_interleave(k)\r\n",
    "    neg_v = torch.randint(0, G.num_nodes(), (len(neg_u),))\r\n",
    "    return dgl.graph((neg_u, neg_v), num_nodes=G.num_nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\r\n",
    "    def __init__(self, n_inputs, n_hidden):\r\n",
    "        super().__init__()\r\n",
    "        self.conv1 = gnn.SAGEConv(n_inputs, n_hidden, aggregator_type='mean', activation=F.relu)\r\n",
    "        self.conv2 = gnn.SAGEConv(n_hidden, n_hidden, aggregator_type='mean')\r\n",
    "        self.predictor = DotPredictor()\r\n",
    "    \r\n",
    "    def forward(self, G, nG, G_features):\r\n",
    "        out = self.conv1(G, G_features)\r\n",
    "        out = self.conv2(G, out)\r\n",
    "        pred_pos = self.predictor(G, out)\r\n",
    "        pred_neg = self.predictor(nG, out)\r\n",
    "        return pred_pos, pred_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(pos_score, neg_score):\r\n",
    "    scores = torch.cat([pos_score, neg_score])\r\n",
    "    labels = torch.cat([torch.ones_like(pos_score), torch.zeros_like(neg_score)])\r\n",
    "    return F.binary_cross_entropy_with_logits(scores, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 0, loss: 0.6945202350616455\n",
      "In epoch 50, loss: 0.6912407279014587\n",
      "In epoch 100, loss: 0.6608276963233948\n",
      "In epoch 150, loss: 0.638744592666626\n",
      "In epoch 200, loss: 0.6267092823982239\n",
      "In epoch 250, loss: 0.6215561032295227\n",
      "In epoch 300, loss: 0.6158761978149414\n",
      "In epoch 350, loss: 0.6108839511871338\n",
      "In epoch 400, loss: 0.6088335514068604\n",
      "In epoch 450, loss: 0.6077801585197449\n",
      "In epoch 500, loss: 0.6053219437599182\n",
      "In epoch 550, loss: 0.604031503200531\n",
      "In epoch 600, loss: 0.6006120443344116\n",
      "In epoch 650, loss: 0.598950207233429\n",
      "In epoch 700, loss: 0.595956027507782\n",
      "In epoch 750, loss: 0.5948807597160339\n",
      "In epoch 800, loss: 0.5922162532806396\n",
      "In epoch 850, loss: 0.5932337045669556\n",
      "In epoch 900, loss: 0.5919910669326782\n",
      "In epoch 950, loss: 0.5914747714996338\n"
     ]
    }
   ],
   "source": [
    "n_inputs = G.ndata['feat'].shape[1]\r\n",
    "n_hidden = 16\r\n",
    "n_epochs = 1000\r\n",
    "\r\n",
    "model = GCN(n_inputs, n_hidden)\r\n",
    "optimizer = optim.Adam(model.parameters(), lr=.001)\r\n",
    "\r\n",
    "for epoch in range(n_epochs):\r\n",
    "    # forward\r\n",
    "    nG = construct_negative_graph(G, k=3)\r\n",
    "    pos_score, neg_score = model(G, nG, G.ndata['feat'])\r\n",
    "    loss = compute_loss(pos_score, neg_score)\r\n",
    "    # backward\r\n",
    "    loss.backward()\r\n",
    "    optimizer.step()\r\n",
    "    optimizer.zero_grad()\r\n",
    "\r\n",
    "    if not epoch % 50:\r\n",
    "        print('In epoch {}, loss: {}'.format(epoch, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b06e6ab994fc15ce23aa05c7ffef0f9130e5f92563bdff97ffc0fa050e903d35"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('gcn': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}