{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import dgl\r\n",
    "import dgl.nn as gnn\r\n",
    "\r\n",
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "import torch.nn.functional as F\r\n",
    "import torch.optim as optim"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Edge classification with neighborhood sampling"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Обучение для задачи предсказания на ребрах похоже на предсказание на узлах. \r\n",
    "1. Для сэмплинга соседей используются все те же самые сэмплеры\r\n",
    "2. Вместо `NodeDataLoader` используем `EdgeDataLoader`, который итерируется по батчам из ребер. Он возвращает:\r\n",
    "* `input_nodes` - узлы, которые необходимы для расчетов по батчу\r\n",
    "* `edge_subgraph` - подграф на основе ребер из батча\r\n",
    "* `blocks` - MFGs для вычислений по слоям\r\n",
    "3. Иногда требуется удалить из графа вычислений те ребра, на которых происходит обучение, иначе модель в теории может использовать факт наличия ребра. В `DGL` мы можем удалить ребра, попавшие в батч, из оригинального графа перед сэмплингом соседей (а также обратные ребра, если это нужно). __Вопрос__: убирает ли он что-то по умолчанию?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Как обычно, модель для задачи классификации ребер состоит из двух частей:\r\n",
    "1. Получение представлений для узлов\r\n",
    "2. Расчет оценки для ребра на основе представлений инцидентных узлов"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# модель из 10_dgl_stochastic_node_classification\r\n",
    "class Conv(nn.Module):\r\n",
    "    def __init__(self, in_features, hidden_features):\r\n",
    "        super().__init__()\r\n",
    "        self.conv1 = gnn.GraphConv(in_features, hidden_features)\r\n",
    "        self.conv2 = gnn.GraphConv(hidden_features, hidden_features)\r\n",
    "\r\n",
    "    def forward(self, blocks, x):\r\n",
    "        x = F.relu(self.conv1(blocks[0], x))\r\n",
    "        x = self.conv2(blocks[1], x)\r\n",
    "        return x  \r\n",
    "\r\n",
    "# обычный MLPPredictor, в forward вместо всего графа\r\n",
    "# придет подграф, созданный на основе батча ребер\r\n",
    "class MLPPredictor(nn.Module):\r\n",
    "    def __init__(self, n_inputs, n_classes):\r\n",
    "        super().__init__()\r\n",
    "        self.linear = nn.Linear(2 * n_inputs, n_classes)\r\n",
    "\r\n",
    "    def get_score(self, edges):\r\n",
    "        data = torch.cat([edges.src['h'], edges.dst['h']], dim=1)\r\n",
    "        return {'score': self.linear(data)}\r\n",
    "\r\n",
    "    def forward(self, edge_subgraph, features):\r\n",
    "        with edge_subgraph.local_scope():\r\n",
    "            edge_subgraph.ndata['h'] = features\r\n",
    "            edge_subgraph.apply_edges(self.get_score)\r\n",
    "            return edge_subgraph.edata['score']  "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "class GCN(nn.Module):\r\n",
    "    def __init__(self, n_inputs, n_hidden, n_classes):\r\n",
    "        super().__init__()\r\n",
    "        self.conv = Conv(n_inputs, n_hidden)\r\n",
    "        self.predictor = MLPPredictor(n_hidden, n_classes)\r\n",
    "\r\n",
    "    def forward(self, edge_subgraph, blocks, features):\r\n",
    "        # blocks - для расчета по всем нужным узлам\r\n",
    "        # edge_subgraph - для расчета по всем нужным связям\r\n",
    "        out = self.conv(blocks, features)\r\n",
    "        return self.predictor(edge_subgraph, out)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "from utils import create_edge_pred_graph\r\n",
    "\r\n",
    "n_nodes, n_edges, n_node_features,n_edge_features = 50, 100, 10, 10\r\n",
    "G = create_edge_pred_graph(n_nodes, n_edges, n_node_features, n_edge_features)\r\n",
    "\r\n",
    "node_features = G.ndata['feature']\r\n",
    "edge_labels = G.edata['label_class'].long()\r\n",
    "train_mask = G.edata['train_mask']\r\n",
    "train_eids = train_mask.nonzero().flatten()\r\n",
    "\r\n",
    "n_classes = len(edge_labels.unique())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "model = GCN(n_node_features, 16, n_classes)\r\n",
    "\r\n",
    "criterion = nn.CrossEntropyLoss()\r\n",
    "optimizer = optim.Adam(model.parameters(), lr=.001)\r\n",
    "\r\n",
    "sampler = dgl.dataloading.MultiLayerFullNeighborSampler(n_layers=2)\r\n",
    "dataloader = dgl.dataloading.EdgeDataLoader(G, train_eids, sampler, batch_size=32, shuffle=True)\r\n",
    "\r\n",
    "for epoch in range(20):\r\n",
    "    for step, (input_nodes, edge_subgraph, blocks) in enumerate(dataloader):\r\n",
    "        batch_features = blocks[0].srcdata['feature']\r\n",
    "        # batch_labels = edge_labels[edge_subgraph.edata['_ID']]\r\n",
    "        batch_labels = edge_subgraph.edata['label_class'].long()\r\n",
    "\r\n",
    "        preds = model(edge_subgraph, blocks, batch_features)\r\n",
    "        loss = criterion(preds, batch_labels)\r\n",
    "\r\n",
    "        loss.backward()\r\n",
    "        optimizer.step()\r\n",
    "        optimizer.zero_grad()\r\n",
    "\r\n",
    "        if not step % 5:\r\n",
    "            acc = (preds.argmax(dim=1) == batch_labels).sum() / len(preds)\r\n",
    "            print('Epoch {:05d} | Step {:05d} | Loss {:.4f} | Train Acc {:.4f}'.format(\r\n",
    "                        epoch, step, loss.item(), acc.item()))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 00000 | Step 00000 | Loss 0.6714 | Train Acc 0.5625\n",
      "Epoch 00001 | Step 00000 | Loss 0.6803 | Train Acc 0.5000\n",
      "Epoch 00002 | Step 00000 | Loss 0.6657 | Train Acc 0.5938\n",
      "Epoch 00003 | Step 00000 | Loss 0.6818 | Train Acc 0.5312\n",
      "Epoch 00004 | Step 00000 | Loss 0.6566 | Train Acc 0.6250\n",
      "Epoch 00005 | Step 00000 | Loss 0.6461 | Train Acc 0.6875\n",
      "Epoch 00006 | Step 00000 | Loss 0.6125 | Train Acc 0.7188\n",
      "Epoch 00007 | Step 00000 | Loss 0.6389 | Train Acc 0.6875\n",
      "Epoch 00008 | Step 00000 | Loss 0.6505 | Train Acc 0.6250\n",
      "Epoch 00009 | Step 00000 | Loss 0.6558 | Train Acc 0.7188\n",
      "Epoch 00010 | Step 00000 | Loss 0.6794 | Train Acc 0.5938\n",
      "Epoch 00011 | Step 00000 | Loss 0.6458 | Train Acc 0.7500\n",
      "Epoch 00012 | Step 00000 | Loss 0.6351 | Train Acc 0.6875\n",
      "Epoch 00013 | Step 00000 | Loss 0.6460 | Train Acc 0.6562\n",
      "Epoch 00014 | Step 00000 | Loss 0.6219 | Train Acc 0.7500\n",
      "Epoch 00015 | Step 00000 | Loss 0.6211 | Train Acc 0.7188\n",
      "Epoch 00016 | Step 00000 | Loss 0.6231 | Train Acc 0.7500\n",
      "Epoch 00017 | Step 00000 | Loss 0.6158 | Train Acc 0.7500\n",
      "Epoch 00018 | Step 00000 | Loss 0.6368 | Train Acc 0.6562\n",
      "Epoch 00019 | Step 00000 | Loss 0.6050 | Train Acc 0.6562\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Heterogenious graph stochastic training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# копия из 10_dgl_stochastic_node_classification\r\n",
    "class RGCN(nn.Module):\r\n",
    "    def __init__(self, n_inputs, n_hidden, rel_names):\r\n",
    "        super().__init__()\r\n",
    "        conv1_modules = {rel: gnn.GraphConv(n_inputs, n_hidden) for rel in rel_names}\r\n",
    "        conv2_modules = {rel: gnn.GraphConv(n_hidden, n_hidden) for rel in rel_names}\r\n",
    "        self.conv1 = gnn.HeteroGraphConv(conv1_modules, aggregate='sum')\r\n",
    "        self.conv2 = gnn.HeteroGraphConv(conv2_modules, aggregate='sum')\r\n",
    "\r\n",
    "    def forward(self, blocks, features):\r\n",
    "        out = self.conv1(blocks[0], features)\r\n",
    "        out = {k: F.relu(v) for k, v in out.items()}\r\n",
    "        out = self.conv2(blocks[1], out)\r\n",
    "        return out\r\n",
    "\r\n",
    "class ScorePredictor(nn.Module):\r\n",
    "    def __init__(self, n_inputs, n_classes):\r\n",
    "        super().__init__()\r\n",
    "        self.W = nn.Linear(2 * n_inputs, n_classes)\r\n",
    "\r\n",
    "    def apply_edges(self, edges):\r\n",
    "        data = torch.cat([edges.src['h'], edges.dst['h']], dim=1)\r\n",
    "        return {'score': self.W(data)}\r\n",
    "\r\n",
    "    def forward(self, edge_subgraph, features):\r\n",
    "        with edge_subgraph.local_scope():\r\n",
    "            edge_subgraph.ndata['h'] = features\r\n",
    "            # итерируемся по всем типам ребер, чтобы получить для всех них оценки\r\n",
    "            for etype in edge_subgraph.canonical_etypes:\r\n",
    "                edge_subgraph.apply_edges(self.apply_edges, etype=etype)\r\n",
    "            return edge_subgraph.edata['score']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "class GCN(nn.Module):\r\n",
    "    def __init__(self, n_inputs, n_hidden, n_classes, etypes):\r\n",
    "        super().__init__()\r\n",
    "        self.rgcn = RGCN(n_inputs, n_hidden, etypes)\r\n",
    "        self.pred = ScorePredictor(n_hidden, n_classes)\r\n",
    "\r\n",
    "    def forward(self, edge_subgraph, blocks, x):\r\n",
    "        x = self.rgcn(blocks, x)\r\n",
    "        return self.pred(edge_subgraph, x)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "from utils import create_heterograph\r\n",
    "\r\n",
    "G = create_heterograph()\r\n",
    "\r\n",
    "n_hetero_features = 10\r\n",
    "rel_names = G.etypes\r\n",
    "\r\n",
    "train_mask = G.edges['click'].data['train_mask']\r\n",
    "train_eids = {'click': G.edges['click'].data['train_mask'].nonzero().flatten()}\r\n",
    "\r\n",
    "n_classes = 4"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "model = GCN(n_hetero_features, 16, n_classes, rel_names)\r\n",
    "\r\n",
    "criterion = nn.CrossEntropyLoss()\r\n",
    "optimizer = optim.Adam(model.parameters(), lr=.001)\r\n",
    "\r\n",
    "n_edges = G.num_edges()\r\n",
    "sampler = dgl.dataloading.MultiLayerFullNeighborSampler(n_layers=2)\r\n",
    "dataloader = dgl.dataloading.EdgeDataLoader(G, train_eids, sampler, batch_size=128, shuffle=True)\r\n",
    "\r\n",
    "for epoch in range(20):\r\n",
    "    for step, (input_nodes, edge_subgraph, blocks) in enumerate(dataloader):\r\n",
    "        batch_features = blocks[0].srcdata['feature']\r\n",
    "        # синтетические классы\r\n",
    "        batch_labels = (edge_subgraph.edges['click'].data['label'] % 4).long()\r\n",
    "\r\n",
    "        preds = model(edge_subgraph, blocks, batch_features)\r\n",
    "        preds_click = preds[('user', 'click', 'item')]\r\n",
    "        loss = criterion(preds_click, batch_labels)\r\n",
    "\r\n",
    "        loss.backward()\r\n",
    "        optimizer.step()\r\n",
    "        optimizer.zero_grad()\r\n",
    "\r\n",
    "        if not step % 5:\r\n",
    "            acc = (preds_click.argmax(dim=1) == batch_labels).sum() / len(preds_click)\r\n",
    "            print('Epoch {:05d} | Step {:05d} | Loss {:.4f} | Train Acc {:.4f}'.format(\r\n",
    "                        epoch, step, loss.item(), acc.item()))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 00000 | Step 00000 | Loss 1.5354 | Train Acc 0.2109\n",
      "Epoch 00000 | Step 00005 | Loss 1.4470 | Train Acc 0.2500\n",
      "Epoch 00000 | Step 00010 | Loss 1.4314 | Train Acc 0.2969\n",
      "Epoch 00000 | Step 00015 | Loss 1.3962 | Train Acc 0.3125\n",
      "Epoch 00000 | Step 00020 | Loss 1.4737 | Train Acc 0.2109\n",
      "Epoch 00001 | Step 00000 | Loss 1.3788 | Train Acc 0.3047\n",
      "Epoch 00001 | Step 00005 | Loss 1.3628 | Train Acc 0.3125\n",
      "Epoch 00001 | Step 00010 | Loss 1.3986 | Train Acc 0.2422\n",
      "Epoch 00001 | Step 00015 | Loss 1.4299 | Train Acc 0.2422\n",
      "Epoch 00001 | Step 00020 | Loss 1.4191 | Train Acc 0.2734\n",
      "Epoch 00002 | Step 00000 | Loss 1.3411 | Train Acc 0.3828\n",
      "Epoch 00002 | Step 00005 | Loss 1.3798 | Train Acc 0.2891\n",
      "Epoch 00002 | Step 00010 | Loss 1.3671 | Train Acc 0.3594\n",
      "Epoch 00002 | Step 00015 | Loss 1.4395 | Train Acc 0.2344\n",
      "Epoch 00002 | Step 00020 | Loss 1.3408 | Train Acc 0.3906\n",
      "Epoch 00003 | Step 00000 | Loss 1.3552 | Train Acc 0.3203\n",
      "Epoch 00003 | Step 00005 | Loss 1.4390 | Train Acc 0.2344\n",
      "Epoch 00003 | Step 00010 | Loss 1.3729 | Train Acc 0.3281\n",
      "Epoch 00003 | Step 00015 | Loss 1.3664 | Train Acc 0.3359\n",
      "Epoch 00003 | Step 00020 | Loss 1.3813 | Train Acc 0.3203\n",
      "Epoch 00004 | Step 00000 | Loss 1.3668 | Train Acc 0.2969\n",
      "Epoch 00004 | Step 00005 | Loss 1.3339 | Train Acc 0.3516\n",
      "Epoch 00004 | Step 00010 | Loss 1.3541 | Train Acc 0.3281\n",
      "Epoch 00004 | Step 00015 | Loss 1.3614 | Train Acc 0.3203\n",
      "Epoch 00004 | Step 00020 | Loss 1.3591 | Train Acc 0.3594\n",
      "Epoch 00005 | Step 00000 | Loss 1.3156 | Train Acc 0.3906\n",
      "Epoch 00005 | Step 00005 | Loss 1.3349 | Train Acc 0.2812\n",
      "Epoch 00005 | Step 00010 | Loss 1.3799 | Train Acc 0.2812\n",
      "Epoch 00005 | Step 00015 | Loss 1.3519 | Train Acc 0.3281\n",
      "Epoch 00005 | Step 00020 | Loss 1.3742 | Train Acc 0.3281\n",
      "Epoch 00006 | Step 00000 | Loss 1.3513 | Train Acc 0.3203\n",
      "Epoch 00006 | Step 00005 | Loss 1.3664 | Train Acc 0.3438\n",
      "Epoch 00006 | Step 00010 | Loss 1.3798 | Train Acc 0.2734\n",
      "Epoch 00006 | Step 00015 | Loss 1.3281 | Train Acc 0.3750\n",
      "Epoch 00006 | Step 00020 | Loss 1.3365 | Train Acc 0.3594\n",
      "Epoch 00007 | Step 00000 | Loss 1.3615 | Train Acc 0.3203\n",
      "Epoch 00007 | Step 00005 | Loss 1.3547 | Train Acc 0.3672\n",
      "Epoch 00007 | Step 00010 | Loss 1.3978 | Train Acc 0.2344\n",
      "Epoch 00007 | Step 00015 | Loss 1.3778 | Train Acc 0.3047\n",
      "Epoch 00007 | Step 00020 | Loss 1.3384 | Train Acc 0.3438\n",
      "Epoch 00008 | Step 00000 | Loss 1.3541 | Train Acc 0.3047\n",
      "Epoch 00008 | Step 00005 | Loss 1.3146 | Train Acc 0.3828\n",
      "Epoch 00008 | Step 00010 | Loss 1.3115 | Train Acc 0.4219\n",
      "Epoch 00008 | Step 00015 | Loss 1.3345 | Train Acc 0.3828\n",
      "Epoch 00008 | Step 00020 | Loss 1.3595 | Train Acc 0.3359\n",
      "Epoch 00009 | Step 00000 | Loss 1.3285 | Train Acc 0.3828\n",
      "Epoch 00009 | Step 00005 | Loss 1.3912 | Train Acc 0.2969\n",
      "Epoch 00009 | Step 00010 | Loss 1.3533 | Train Acc 0.3047\n",
      "Epoch 00009 | Step 00015 | Loss 1.3326 | Train Acc 0.4219\n",
      "Epoch 00009 | Step 00020 | Loss 1.3643 | Train Acc 0.3047\n",
      "Epoch 00010 | Step 00000 | Loss 1.3490 | Train Acc 0.3203\n",
      "Epoch 00010 | Step 00005 | Loss 1.3167 | Train Acc 0.3672\n",
      "Epoch 00010 | Step 00010 | Loss 1.2889 | Train Acc 0.4375\n",
      "Epoch 00010 | Step 00015 | Loss 1.3785 | Train Acc 0.2969\n",
      "Epoch 00010 | Step 00020 | Loss 1.3219 | Train Acc 0.3594\n",
      "Epoch 00011 | Step 00000 | Loss 1.3407 | Train Acc 0.3594\n",
      "Epoch 00011 | Step 00005 | Loss 1.3478 | Train Acc 0.3047\n",
      "Epoch 00011 | Step 00010 | Loss 1.3595 | Train Acc 0.3125\n",
      "Epoch 00011 | Step 00015 | Loss 1.3213 | Train Acc 0.3594\n",
      "Epoch 00011 | Step 00020 | Loss 1.3853 | Train Acc 0.3281\n",
      "Epoch 00012 | Step 00000 | Loss 1.3319 | Train Acc 0.3672\n",
      "Epoch 00012 | Step 00005 | Loss 1.3755 | Train Acc 0.3125\n",
      "Epoch 00012 | Step 00010 | Loss 1.3564 | Train Acc 0.3516\n",
      "Epoch 00012 | Step 00015 | Loss 1.3257 | Train Acc 0.3438\n",
      "Epoch 00012 | Step 00020 | Loss 1.3188 | Train Acc 0.4219\n",
      "Epoch 00013 | Step 00000 | Loss 1.3423 | Train Acc 0.3359\n",
      "Epoch 00013 | Step 00005 | Loss 1.3612 | Train Acc 0.3438\n",
      "Epoch 00013 | Step 00010 | Loss 1.3272 | Train Acc 0.3516\n",
      "Epoch 00013 | Step 00015 | Loss 1.3405 | Train Acc 0.3516\n",
      "Epoch 00013 | Step 00020 | Loss 1.2987 | Train Acc 0.3984\n",
      "Epoch 00014 | Step 00000 | Loss 1.3465 | Train Acc 0.2891\n",
      "Epoch 00014 | Step 00005 | Loss 1.3161 | Train Acc 0.3516\n",
      "Epoch 00014 | Step 00010 | Loss 1.3210 | Train Acc 0.4375\n",
      "Epoch 00014 | Step 00015 | Loss 1.3407 | Train Acc 0.3438\n",
      "Epoch 00014 | Step 00020 | Loss 1.3375 | Train Acc 0.3750\n",
      "Epoch 00015 | Step 00000 | Loss 1.3001 | Train Acc 0.4531\n",
      "Epoch 00015 | Step 00005 | Loss 1.3244 | Train Acc 0.3828\n",
      "Epoch 00015 | Step 00010 | Loss 1.2776 | Train Acc 0.4297\n",
      "Epoch 00015 | Step 00015 | Loss 1.3406 | Train Acc 0.3516\n",
      "Epoch 00015 | Step 00020 | Loss 1.3326 | Train Acc 0.3359\n",
      "Epoch 00016 | Step 00000 | Loss 1.3631 | Train Acc 0.3125\n",
      "Epoch 00016 | Step 00005 | Loss 1.3273 | Train Acc 0.4141\n",
      "Epoch 00016 | Step 00010 | Loss 1.3396 | Train Acc 0.3125\n",
      "Epoch 00016 | Step 00015 | Loss 1.3401 | Train Acc 0.3750\n",
      "Epoch 00016 | Step 00020 | Loss 1.3000 | Train Acc 0.4062\n",
      "Epoch 00017 | Step 00000 | Loss 1.3321 | Train Acc 0.3828\n",
      "Epoch 00017 | Step 00005 | Loss 1.2660 | Train Acc 0.4609\n",
      "Epoch 00017 | Step 00010 | Loss 1.3612 | Train Acc 0.3203\n",
      "Epoch 00017 | Step 00015 | Loss 1.3092 | Train Acc 0.4062\n",
      "Epoch 00017 | Step 00020 | Loss 1.3628 | Train Acc 0.3359\n",
      "Epoch 00018 | Step 00000 | Loss 1.3028 | Train Acc 0.4219\n",
      "Epoch 00018 | Step 00005 | Loss 1.2947 | Train Acc 0.3828\n",
      "Epoch 00018 | Step 00010 | Loss 1.3464 | Train Acc 0.3203\n",
      "Epoch 00018 | Step 00015 | Loss 1.3461 | Train Acc 0.2969\n",
      "Epoch 00018 | Step 00020 | Loss 1.3392 | Train Acc 0.3438\n",
      "Epoch 00019 | Step 00000 | Loss 1.2765 | Train Acc 0.4609\n",
      "Epoch 00019 | Step 00005 | Loss 1.3423 | Train Acc 0.3750\n",
      "Epoch 00019 | Step 00010 | Loss 1.2935 | Train Acc 0.3828\n",
      "Epoch 00019 | Step 00015 | Loss 1.3365 | Train Acc 0.3594\n",
      "Epoch 00019 | Step 00020 | Loss 1.3103 | Train Acc 0.3516\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('gcn': conda)"
  },
  "interpreter": {
   "hash": "b06e6ab994fc15ce23aa05c7ffef0f9130e5f92563bdff97ffc0fa050e903d35"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}