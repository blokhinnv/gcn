{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import dgl.data\n",
    "import dgl.nn as gnn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homogenious graph node classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Датасет Cora: \n",
    "* узлы - статьи\n",
    "* связи - цитирование одной статьей другой\n",
    "* каждый узел в качестве фичей содержит нормализованный word count vector \n",
    "\n",
    "Датасет может состоять из одного или нескольких графов. Cora состоит из одного.\n",
    "\n",
    "Граф в DGL может хранить фичи для узлов и ребер в виде словарей `ndata` и `edata`. \n",
    "\n",
    "Фичи узлов в Cora:\n",
    "* x_mask - булев тензор, показывающий, входит ли узел в множество x (train, val, test)\n",
    "* label - метка узла\n",
    "* feat - фичи узла"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NumNodes: 2708\n",
      "  NumEdges: 10556\n",
      "  NumFeats: 1433\n",
      "  NumClasses: 7\n",
      "  NumTrainingSamples: 140\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done loading data from cached files.\n",
      "Кол-во категорий: 7\n"
     ]
    }
   ],
   "source": [
    "dataset = dgl.data.CoraGraphDataset()\n",
    "G = dataset[0]\n",
    "\n",
    "print(f\"Кол-во категорий: {dataset.num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, n_input, n_hidden, n_output):\n",
    "        super().__init__()\n",
    "        self.conv1 = gnn.GraphConv(n_input, n_hidden)\n",
    "        self.conv2 = gnn.GraphConv(n_hidden, n_output)\n",
    "    \n",
    "    def forward(self, G, in_features):\n",
    "        out = F.relu(self.conv1(G, in_features))\n",
    "        out = self.conv2(G, out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input = G.ndata['feat'].shape[1]\n",
    "n_hidden = 16\n",
    "n_out = dataset.num_classes\n",
    "n_epochs = 100\n",
    "\n",
    "model = GCN(n_input, n_hidden, n_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2708, 7])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check\n",
    "model(G, G.ndata['feat']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 0, loss: 1.945, val acc: 0.172 (best 0.172), test acc: 0.159 (best 0.159)\n",
      "In epoch 5, loss: 1.886, val acc: 0.516 (best 0.540), test acc: 0.527 (best 0.546)\n",
      "In epoch 10, loss: 1.804, val acc: 0.590 (best 0.590), test acc: 0.612 (best 0.612)\n",
      "In epoch 15, loss: 1.699, val acc: 0.622 (best 0.622), test acc: 0.631 (best 0.631)\n",
      "In epoch 20, loss: 1.572, val acc: 0.636 (best 0.636), test acc: 0.646 (best 0.646)\n",
      "In epoch 25, loss: 1.425, val acc: 0.644 (best 0.646), test acc: 0.657 (best 0.656)\n",
      "In epoch 30, loss: 1.263, val acc: 0.658 (best 0.658), test acc: 0.673 (best 0.668)\n",
      "In epoch 35, loss: 1.094, val acc: 0.692 (best 0.692), test acc: 0.694 (best 0.694)\n",
      "In epoch 40, loss: 0.927, val acc: 0.704 (best 0.704), test acc: 0.712 (best 0.712)\n",
      "In epoch 45, loss: 0.771, val acc: 0.718 (best 0.718), test acc: 0.725 (best 0.723)\n",
      "In epoch 50, loss: 0.632, val acc: 0.730 (best 0.730), test acc: 0.740 (best 0.740)\n",
      "In epoch 55, loss: 0.515, val acc: 0.754 (best 0.754), test acc: 0.761 (best 0.761)\n",
      "In epoch 60, loss: 0.417, val acc: 0.758 (best 0.758), test acc: 0.768 (best 0.765)\n",
      "In epoch 65, loss: 0.338, val acc: 0.770 (best 0.770), test acc: 0.777 (best 0.777)\n",
      "In epoch 70, loss: 0.274, val acc: 0.780 (best 0.780), test acc: 0.784 (best 0.784)\n",
      "In epoch 75, loss: 0.224, val acc: 0.786 (best 0.786), test acc: 0.789 (best 0.789)\n",
      "In epoch 80, loss: 0.184, val acc: 0.794 (best 0.794), test acc: 0.789 (best 0.789)\n",
      "In epoch 85, loss: 0.152, val acc: 0.794 (best 0.794), test acc: 0.790 (best 0.789)\n",
      "In epoch 90, loss: 0.127, val acc: 0.794 (best 0.794), test acc: 0.794 (best 0.789)\n",
      "In epoch 95, loss: 0.107, val acc: 0.790 (best 0.794), test acc: 0.795 (best 0.789)\n"
     ]
    }
   ],
   "source": [
    "model = GCN(n_input, n_hidden, n_out)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "best_val_acc, best_test_acc = 0, 0\n",
    "\n",
    "\n",
    "features = G.ndata['feat']\n",
    "labels = G.ndata['label']\n",
    "train_mask = G.ndata['train_mask']\n",
    "val_mask = G.ndata['val_mask']\n",
    "test_mask = G.ndata['test_mask']\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # forward\n",
    "    logits = model(G, features)\n",
    "    \n",
    "    # loss\n",
    "    loss = criterion(logits[train_mask], labels[train_mask])\n",
    "\n",
    "    # backward\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # eval\n",
    "    with torch.no_grad():\n",
    "        predictions = logits.argmax(dim=1)\n",
    "        train_acc = (predictions[train_mask] == labels[train_mask]).float().mean()\n",
    "        val_acc = (predictions[val_mask] == labels[val_mask]).float().mean()\n",
    "        test_acc = (predictions[test_mask] == labels[test_mask]).float().mean()\n",
    "\n",
    "        if best_val_acc < val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "\n",
    "    if not epoch % 5:\n",
    "        print(f'In epoch {epoch}, loss: {loss:.3f}, val acc: {val_acc:.3f} (best {best_val_acc:.3f}), test acc: {test_acc:.3f} (best {best_test_acc:.3f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heterogenious graph node classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для гетерографов можно использовать модули, которые позволяют собирать сообщения от узлов вдоль всех типов связей. Пример: `HeteroGraphConv`. С его помощью выполняем рассылку сообщений по типам связей, затем комбинируем различные сверточные модули для каждого типа связи.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RGCN(nn.Module):\n",
    "    def __init__(self, n_inputs, n_hidden, n_outputs, rel_names):\n",
    "        super().__init__()\n",
    "        # HeteroGraphConv использует различные подмодули для подграфов на \n",
    "        # основе соответствующих отношений\n",
    "        # отношение определяется тройкой (src_T, rel_T, dst_T)\n",
    "        # если для каких-то отношений используются одинаковые dst_T,\n",
    "        # то результаты для них будут сагрегированы указанным методом aggregate\n",
    "        conv1_modules = {rel: gnn.GraphConv(n_inputs, n_hidden) for rel in rel_names}\n",
    "        conv2_modules = {rel: gnn.GraphConv(n_hidden, n_outputs) for rel in rel_names}\n",
    "        self.conv1 = gnn.HeteroGraphConv(conv1_modules, aggregate='sum')\n",
    "        self.conv2 = gnn.HeteroGraphConv(conv2_modules, aggregate='sum')\n",
    "\n",
    "    def forward(self, G, features):\n",
    "        # HeteroGraphConv принимает на вход словарь тип отношения: фичи узлов и \n",
    "        # возвращает словарь такой же структуры\n",
    "        out = self.conv1(G, features)\n",
    "        out = {k: F.relu(v) for k, v in out.items()}\n",
    "        out = self.conv2(G, out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0 loss=1.8041772842407227\n",
      "Epoch #1 loss=1.795522928237915\n",
      "Epoch #2 loss=1.7874788045883179\n",
      "Epoch #3 loss=1.7800294160842896\n",
      "Epoch #4 loss=1.7731542587280273\n"
     ]
    }
   ],
   "source": [
    "from utils import create_heterograph\n",
    "\n",
    "G = create_heterograph()\n",
    "model = RGCN(G.n_hetero_features, \n",
    "             20, \n",
    "             G.n_user_classes,\n",
    "             G.etypes)\n",
    "\n",
    "user_feats = G.nodes['user'].data['feature']\n",
    "item_feats = G.nodes['item'].data['feature']\n",
    "node_features = {ntype: G.nodes[ntype].data['feature'] for ntype in G.ntypes}\n",
    "labels = G.nodes['user'].data['label']\n",
    "train_mask = G.nodes['user'].data['train_mask']\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    # forward\n",
    "    logits_by_type = model(G, node_features)\n",
    "    # обучаемся только на пользователях\n",
    "    logits = logits_by_type['user']\n",
    "    loss = criterion(logits, labels)\n",
    "    # backward\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    print(f'Epoch #{epoch} loss={loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RGCN(nn.Module):\n",
    "    def __init__(self, n_inputs, n_hidden, n_outputs, num_rels, dropout=0):\n",
    "        super().__init__()\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_outputs = n_outputs\n",
    "\n",
    "        self.conv1 = gnn.RelGraphConv(n_inputs, n_hidden, num_rels, \n",
    "                                      activation=F.relu, self_loop=True,\n",
    "                                      dropout=dropout)\n",
    "        self.conv2 = gnn.RelGraphConv(n_hidden, n_hidden, num_rels, \n",
    "                                      activation=F.relu, self_loop=True,\n",
    "                                      dropout=dropout)                              \n",
    "        self.conv3 = gnn.RelGraphConv(n_hidden, n_outputs, num_rels, \n",
    "                                      activation=None, self_loop=True)                              \n",
    "\n",
    "    def forward(self, G, features, etypes):\n",
    "        out = self.conv1(G, features, etypes)\n",
    "        out = self.conv2(G, out, etypes)\n",
    "        out = self.conv3(G, out, etypes)\n",
    "        return out\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, n_inputs, n_hidden, n_outputs):\n",
    "        super().__init__()\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_outputs = n_outputs\n",
    "\n",
    "        self.conv1 = gnn.SAGEConv(n_inputs, n_hidden, aggregator_type='mean', \n",
    "                                  activation=F.relu)\n",
    "        self.conv2 = gnn.SAGEConv(n_hidden, n_hidden, aggregator_type='mean', \n",
    "                                  activation=F.relu)                              \n",
    "        self.conv3 = gnn.SAGEConv(n_hidden, n_outputs, aggregator_type='mean')                              \n",
    "\n",
    "    def forward(self, G, features):\n",
    "        out = self.conv1(G, features)\n",
    "        out = self.conv2(G, out)\n",
    "        out = self.conv3(G, out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done loading data from cached files.\n",
      "G.num_nodes(category)=237\n"
     ]
    }
   ],
   "source": [
    "from dgl.data.rdf import AIFBDataset\n",
    "\n",
    "dataset = AIFBDataset()\n",
    "G = dataset[0]\n",
    "num_classes = dataset.num_classes\n",
    "\n",
    "# обучаемся только на этой категории\n",
    "category = dataset.predict_category #  Personnen\n",
    "category_id = G.ntypes.index(category)\n",
    "num_rels = len(G.etypes)\n",
    "print(f'{G.num_nodes(category)=}')\n",
    "\n",
    "# маски\n",
    "for ntype in G.ntypes:\n",
    "    if ntype != category:\n",
    "        G.nodes[ntype].data['train_mask'] = torch.zeros(G.num_nodes(ntype), dtype=torch.uint8)\n",
    "        G.nodes[ntype].data['test_mask'] = torch.zeros(G.num_nodes(ntype), dtype=torch.uint8)\n",
    "        G.nodes[ntype].data['labels'] = torch.zeros(G.num_nodes(ntype), dtype=torch.int64)\n",
    "    G.nodes[ntype].data['features'] = torch.ones((G.num_nodes(ntype), 1), dtype=torch.float32)\n",
    "\n",
    "# чтобы воспользоваться RelGraphConv перехожу к однородному графу\n",
    "G = dgl.to_homogeneous(G, ndata=['features', 'train_mask', 'labels', 'test_mask'])\n",
    "G = dgl.add_self_loop(G)\n",
    "\n",
    "train_mask = G.ndata['train_mask'].bool()\n",
    "test_mask = G.ndata['test_mask'].bool()\n",
    "train_size = len(train_mask.nonzero())\n",
    "test_size = len(test_mask.nonzero())\n",
    "\n",
    "labels = G.ndata['labels'] # содержат -1, но они не попадают под маски\n",
    "features = G.ndata['features']\n",
    "etypes = G.edata['_TYPE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000 | Train Acc: 0.2786 | Train Loss: 53.3823 | Test Acc: 0.2778\n",
      "Epoch 00005 | Train Acc: 0.6071 | Train Loss: 2.1778 | Test Acc: 0.5556\n",
      "Epoch 00010 | Train Acc: 0.6929 | Train Loss: 1.6998 | Test Acc: 0.6667\n",
      "Epoch 00015 | Train Acc: 0.7714 | Train Loss: 1.5508 | Test Acc: 0.7222\n",
      "Epoch 00020 | Train Acc: 0.8143 | Train Loss: 1.3119 | Test Acc: 0.7500\n",
      "Epoch 00025 | Train Acc: 0.8071 | Train Loss: 1.0918 | Test Acc: 0.7500\n",
      "Epoch 00030 | Train Acc: 0.8214 | Train Loss: 0.8326 | Test Acc: 0.7500\n",
      "Epoch 00035 | Train Acc: 0.8571 | Train Loss: 0.5455 | Test Acc: 0.8333\n",
      "Epoch 00040 | Train Acc: 0.8714 | Train Loss: 0.3491 | Test Acc: 0.8611\n",
      "Epoch 00045 | Train Acc: 0.8929 | Train Loss: 0.2816 | Test Acc: 0.8889\n",
      "Epoch 00050 | Train Acc: 0.9286 | Train Loss: 0.2489 | Test Acc: 0.8611\n",
      "Epoch 00055 | Train Acc: 0.9429 | Train Loss: 0.2266 | Test Acc: 0.8889\n",
      "Epoch 00060 | Train Acc: 0.9500 | Train Loss: 0.2192 | Test Acc: 0.8889\n",
      "Epoch 00065 | Train Acc: 0.9500 | Train Loss: 0.2118 | Test Acc: 0.8889\n",
      "Epoch 00070 | Train Acc: 0.9500 | Train Loss: 0.2049 | Test Acc: 0.8889\n",
      "Epoch 00075 | Train Acc: 0.9429 | Train Loss: 0.1977 | Test Acc: 0.8333\n",
      "Epoch 00080 | Train Acc: 0.9500 | Train Loss: 0.1916 | Test Acc: 0.8611\n",
      "Epoch 00085 | Train Acc: 0.9500 | Train Loss: 0.1862 | Test Acc: 0.8611\n",
      "Epoch 00090 | Train Acc: 0.9500 | Train Loss: 0.1811 | Test Acc: 0.8611\n",
      "Epoch 00095 | Train Acc: 0.9500 | Train Loss: 0.1763 | Test Acc: 0.8611\n"
     ]
    }
   ],
   "source": [
    "model = RGCN(n_inputs=1, n_hidden=20, \n",
    "            n_outputs=num_classes,\n",
    "            num_rels=num_rels)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    # forward\n",
    "    logits = model(G, features, etypes)\n",
    "    loss = criterion(logits[train_mask], labels[train_mask])\n",
    "    # backward\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if not epoch % 5:\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            predictions = logits.argmax(dim=1)\n",
    "            train_acc = (predictions[train_mask] == labels[train_mask]).sum().item() / train_size\n",
    "            test_acc = (predictions[test_mask] == labels[test_mask]).sum().item() / test_size\n",
    "        print(\"Epoch {:05d} | Train Acc: {:.4f} | Train Loss: {:.4f} | Test Acc: {:.4f}\".\n",
    "              format(epoch, train_acc, loss.item(), test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000 | Train Acc: 0.0857 | Train Loss: 1.4893 | Test Acc: 0.0833\n",
      "Epoch 00005 | Train Acc: 0.4143 | Train Loss: 1.3047 | Test Acc: 0.4167\n",
      "Epoch 00010 | Train Acc: 0.4143 | Train Loss: 1.2448 | Test Acc: 0.4167\n",
      "Epoch 00015 | Train Acc: 0.4143 | Train Loss: 1.2370 | Test Acc: 0.4167\n",
      "Epoch 00020 | Train Acc: 0.4143 | Train Loss: 1.2373 | Test Acc: 0.4167\n",
      "Epoch 00025 | Train Acc: 0.4143 | Train Loss: 1.2400 | Test Acc: 0.4167\n",
      "Epoch 00030 | Train Acc: 0.4143 | Train Loss: 1.2387 | Test Acc: 0.4167\n",
      "Epoch 00035 | Train Acc: 0.4143 | Train Loss: 1.2362 | Test Acc: 0.4167\n",
      "Epoch 00040 | Train Acc: 0.4143 | Train Loss: 1.2345 | Test Acc: 0.4167\n",
      "Epoch 00045 | Train Acc: 0.4143 | Train Loss: 1.2335 | Test Acc: 0.4167\n",
      "Epoch 00050 | Train Acc: 0.4143 | Train Loss: 1.2336 | Test Acc: 0.4167\n",
      "Epoch 00055 | Train Acc: 0.4143 | Train Loss: 1.2337 | Test Acc: 0.4167\n",
      "Epoch 00060 | Train Acc: 0.4143 | Train Loss: 1.2337 | Test Acc: 0.4167\n",
      "Epoch 00065 | Train Acc: 0.4143 | Train Loss: 1.2336 | Test Acc: 0.4167\n",
      "Epoch 00070 | Train Acc: 0.4143 | Train Loss: 1.2335 | Test Acc: 0.4167\n",
      "Epoch 00075 | Train Acc: 0.4143 | Train Loss: 1.2335 | Test Acc: 0.4167\n",
      "Epoch 00080 | Train Acc: 0.4143 | Train Loss: 1.2335 | Test Acc: 0.4167\n",
      "Epoch 00085 | Train Acc: 0.4143 | Train Loss: 1.2335 | Test Acc: 0.4167\n",
      "Epoch 00090 | Train Acc: 0.4143 | Train Loss: 1.2335 | Test Acc: 0.4167\n",
      "Epoch 00095 | Train Acc: 0.4143 | Train Loss: 1.2335 | Test Acc: 0.4167\n",
      "Epoch 00100 | Train Acc: 0.4143 | Train Loss: 1.2335 | Test Acc: 0.4167\n"
     ]
    }
   ],
   "source": [
    "model = GCN(n_inputs=1, n_hidden=50, n_outputs=num_classes)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(101):\n",
    "    model.train()\n",
    "    # forward\n",
    "    logits = model(G, features)\n",
    "    loss = criterion(logits[train_mask], labels[train_mask])\n",
    "    # backward\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if not epoch % 5:\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            predictions = logits.argmax(dim=1)\n",
    "            train_acc = (predictions[train_mask] == labels[train_mask]).sum().item() / train_size\n",
    "            test_acc = (predictions[test_mask] == labels[test_mask]).sum().item() / test_size\n",
    "        print(\"Epoch {:05d} | Train Acc: {:.4f} | Train Loss: {:.4f} | Test Acc: {:.4f}\".\n",
    "              format(epoch, train_acc, loss.item(), test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b06e6ab994fc15ce23aa05c7ffef0f9130e5f92563bdff97ffc0fa050e903d35"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('gcn': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
